{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JannaBabicheva/LAB_1/blob/main/Babicheva___%22embedding_based_MT_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eulvfJWl7ueY"
      },
      "source": [
        "# Lab 1\n",
        "\n",
        "\n",
        "## Part 1: Bilingual dictionary induction and unsupervised embedding-based MT (30%)\n",
        "*Note: this homework is based on materials from yandexdataschool [NLP course](https://github.com/yandexdataschool/nlp_course/). Feel free to check this awesome course if you wish to dig deeper.*\n",
        "\n",
        "*Refined by [Nikolay Karpachev](https://www.linkedin.com/in/nikolay-karpachev-b0146a104/), [Valery Marchenkov](https://www.linkedin.com/in/vmarchenkoff/)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV4rIjxa7uei"
      },
      "source": [
        "**In this homework** **<font color='red'>YOU</font>** will make machine translation system without using parallel corpora, alignment, attention, 100500 depth super-cool recurrent neural network and all that kind superstuff.\n",
        "\n",
        "But even without parallel corpora this system can be good enough (hopefully), in particular for similar languages."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лабораторная работа 1\n",
        "Часть 1: Введение в двуязычный словарь и самостоятельное внедрение на основе MT (30%) (основе неконтролируемого встраивания (30%))\n",
        "Примечание: это домашнее задание основано на материалах курса НЛП yandexdata school. Не стесняйтесь ознакомиться с этим потрясающим курсом, если хотите копнуть глубже.\n",
        "Доработано Николаем Карпачевым, Валерием Марченковым\n",
        "В этом домашнем задании ВЫ создадите систему машинного перевода без использования параллельных корпусов, выравнивания, внимания, супер-крутой рекуррентной нейронной сети глубиной 100 на 500 градусов и всего такого прочего.\n",
        "\n",
        "Но даже без параллельных корпусов эта система может быть достаточно хороша (надеюсь), особенно для похожих языков.\n",
        "\n"
      ],
      "metadata": {
        "id": "BZKhg02quNj1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idSYq2GU7uew"
      },
      "source": [
        "### Frament of the Swadesh list for some slavic languages\n",
        "\n",
        "Фрагмент списка Свадеша для некоторых славянских языков\n",
        "Список Свадеша - это лексикостатистический материал. Он назван в честь американского лингвиста Морриса Свадеша и содержит основную лексику. Этот список используется для определения подгруппы языков, их родства.\n",
        "Таким образом, мы можем видеть некоторую инвариантность слов для разных славянских языков.\n",
        "\n",
        "\n",
        "| Russian         | Belorussian              | Ukrainian               | Polish             | Czech                         | Bulgarian            |\n",
        "|-----------------|--------------------------|-------------------------|--------------------|-------------------------------|-----------------------|\n",
        "| женщина         | жанчына, кабета, баба    | жінка                   | kobieta            | žena                          | жена                  |\n",
        "| мужчина         | мужчына                  | чоловік, мужчина        | mężczyzna          | muž                           | мъж                   |\n",
        "| человек         | чалавек                  | людина, чоловік         | człowiek           | člověk                        | човек                 |\n",
        "| ребёнок, дитя   | дзіця, дзіцёнак, немаўля | дитина, дитя            | dziecko            | dítě                          | дете                  |\n",
        "| жена            | жонка                    | дружина, жінка          | żona               | žena, manželka, choť          | съпруга, жена         |\n",
        "| муж             | муж, гаспадар            | чоловiк, муж            | mąż                | muž, manžel, choť             | съпруг, мъж           |\n",
        "| мать, мама      | маці, матка              | мати, матір, неня, мама | matka              | matka, máma, 'стар.' mateř    | майка                 |\n",
        "| отец, тятя      | бацька, тата             | батько, тато, татусь    | ojciec             | otec                          | баща, татко           |\n",
        "| много           | шмат, багата             | багато                  | wiele              | mnoho, hodně                  | много                 |\n",
        "| несколько       | некалькі, колькі         | декілька, кілька        | kilka              | několik, pár, trocha          | няколко               |\n",
        "| другой, иной    | іншы                     | інший                   | inny               | druhý, jiný                   | друг                  |\n",
        "| зверь, животное | жывёла, звер, істота     | тварина, звір           | zwierzę            | zvíře                         | животно               |\n",
        "| рыба            | рыба                     | риба                    | ryba               | ryba                          | риба                  |\n",
        "| птица           | птушка                   | птах, птиця             | ptak               | pták                          | птица                 |\n",
        "| собака, пёс     | сабака                   | собака, пес             | pies               | pes                           | куче, пес             |\n",
        "| вошь            | вош                      | воша                    | wesz               | veš                           | въшка                 |\n",
        "| змея, гад       | змяя                     | змія, гад               | wąż                | had                           | змия                  |\n",
        "| червь, червяк   | чарвяк                   | хробак, черв'як         | robak              | červ                          | червей                |\n",
        "| дерево          | дрэва                    | дерево                  | drzewo             | strom, dřevo                  | дърво                 |\n",
        "| лес             | лес                      | ліс                     | las                | les                           | гора, лес             |\n",
        "| палка           | кій, палка               | палиця                  | patyk, pręt, pałka | hůl, klacek, prut, kůl, pálka | палка, пръчка, бастун |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNM3_fjr7ue2"
      },
      "source": [
        "Но контекстное распределение этих языков демонстрирует еще большую инвариантность. И мы можем использовать этот факт для наших целей.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLppwa527ue6"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwGoVhRA7ufP"
      },
      "source": [
        "В этом блокноте мы будем использовать предварительно обученные векторы слов - FastText (оригинальная статья - https://arxiv.org/abs/1607.04606).\n",
        "Вы можете скачать их с официального сайта. Нам понадобятся вложения для английского и французского языков.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV2-MpR-ugq-",
        "outputId": "931df24e-6260-427e-aff0-13105ffbe4ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-23 14:02:22--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.172.170.31, 18.172.170.125, 18.172.170.86, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.172.170.31|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.en.300.vec.gz’\n",
            "\n",
            "cc.en.300.vec.gz    100%[===================>]   1.23G   124MB/s    in 14s     \n",
            "\n",
            "2025-03-23 14:02:36 (92.7 MB/s) - ‘cc.en.300.vec.gz’ saved [1325960915/1325960915]\n",
            "\n",
            "--2025-03-23 14:03:28--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.65.229.95, 18.65.229.89, 18.65.229.121, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.65.229.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1287757366 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.fr.300.vec.gz’\n",
            "\n",
            "cc.fr.300.vec.gz    100%[===================>]   1.20G   230MB/s    in 5.4s    \n",
            "\n",
            "2025-03-23 14:03:34 (227 MB/s) - ‘cc.fr.300.vec.gz’ saved [1287757366/1287757366]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#ячейка выполняется 3-ей (т.к. проблемы с загрузкой модуля gensim)\n",
        "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!gzip -d cc.en.300.vec.gz\n",
        "\n",
        "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\n",
        "!gzip -d cc.fr.300.vec.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwg26PKLv88U"
      },
      "source": [
        "After downloading and extracting the vectors, we should be able to load them using the [gensim](https://radimrehurek.com/gensim/) library:\n",
        "\n",
        "После загрузки и извлечения векторов мы сможем загрузить их с помощью библиотеки gensim:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!new_env/Scripts/activate  # выполняем и перезаускаем среду и выполняем второй раз\n",
        "!pip install numpy==1.24.3 gensim==4.3.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL9FogAJDgLi",
        "outputId": "66be2e43-07ca-42cb-e99b-f92fc332c5e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: new_env/Scripts/activate: No such file or directory\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.11/dist-packages (1.24.3)\n",
            "Requirement already satisfied: gensim==4.3.2 in /usr/local/lib/python3.11/dist-packages (4.3.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2) (1.14.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3.2) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade numpy scipy gensim\n",
        "# снова перезапускаем среду и выполняем эту ячейку 2 раз"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT3UrqXcD8gy",
        "outputId": "754f57eb-55fc-4fde-9350-bc3c2a1a7938"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Collecting scipy\n",
            "  Using cached scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Импорт необходимых библиотек\n",
        "from gensim.models import KeyedVectors\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "p084YsGY6iDe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "u1JjQv_97ufT"
      },
      "outputs": [],
      "source": [
        "en_emb = KeyedVectors.load_word2vec_format(\"cc.en.300.vec\")\n",
        "fr_emb = KeyedVectors.load_word2vec_format(\"cc.fr.300.vec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqb_XJhkMyHM"
      },
      "source": [
        "Once you've loaded the vectors, you can use the `KeyedVectors` interface to get word embeddings and/or query most similar words by embedding:\n",
        "\n",
        "После загрузки векторов вы можете использовать интерфейс KeyedVectors для получения вложений слов и/или запроса наиболее похожих слов путем встраивания:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nTkXfT0W7ufk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb56861-6612-4d35-be98-29f5aa5f4e09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((300,), array([-0.0522,  0.0364, -0.1252,  0.0053,  0.0382], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "august_embedding = en_emb[\"august\"]\n",
        "august_embedding.shape, august_embedding[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oQ2kCq-7NQPn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0079b7f6-84cc-4ba9-dcf5-c99157a56353"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('august', 0.9999999403953552),\n",
              " ('september', 0.8252838850021362),\n",
              " ('october', 0.8111193180084229),\n",
              " ('june', 0.8050147891044617),\n",
              " ('july', 0.797055184841156),\n",
              " ('november', 0.788363516330719),\n",
              " ('february', 0.7831973433494568),\n",
              " ('december', 0.7824540138244629),\n",
              " ('january', 0.7743154168128967),\n",
              " ('april', 0.7621643543243408)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "en_emb.most_similar([august_embedding])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5EcMMI6pxzL"
      },
      "source": [
        "The latter function also allows you to vary the amount of closest words via the `topn` argument:\n",
        "\n",
        "Последняя функция также позволяет изменять количество ближайших слов с помощью аргумента topn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bi6AF3z0p9Oo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "931de9a1-0033-4059-ada6-71f7a6e2a9c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('august', 0.9999999403953552),\n",
              " ('september', 0.8252838850021362),\n",
              " ('october', 0.8111193180084229)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "en_emb.most_similar([august_embedding], topn=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw345NRXov4p"
      },
      "source": [
        "Another feature of `KeyedVectors` is that it allows to compute embeddings for multiple words simultaneously:\n",
        "\n",
        "Еще одной особенностью KeyedVectors является то, что он позволяет вычислять вложения для нескольких слов одновременно:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "86OuYeLYow0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66f82b11-49e9-4121-ce65-117666ff35d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "en_emb[[\"august\", \"september\"]].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uGx5zHXQtfo"
      },
      "source": [
        "Everything above is true for the embeddings for French language.\n",
        "\n",
        "Все вышесказанное справедливо для вложений для французского языка."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vdBA8lcg7ufs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40cd2f3c-a2fc-4ec3-81e0-9b97fa29c32e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('aout', 1.0),\n",
              " ('Aout', 0.8249964118003845),\n",
              " ('juillet', 0.8109882473945618),\n",
              " ('fevrier', 0.8072442412376404),\n",
              " ('septembre', 0.7838520407676697),\n",
              " ('août', 0.779176652431488),\n",
              " ('juin', 0.7692081332206726),\n",
              " ('octobre', 0.7597455382347107),\n",
              " ('decembre', 0.7595790028572083),\n",
              " ('avril', 0.7390779256820679)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "fr_emb.most_similar([fr_emb[\"aout\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1Dkka5uQ37-"
      },
      "source": [
        "However, french and english embeddings were trained independently of each other. This means, that there is no obvious connection between values in embeddings for similar words in French and English:\n",
        "\n",
        "Однако французские и английские вложения были обучены независимо друг от друга. Это означает, что нет очевидной связи между значениями в вложениях для похожих слов на французском и английском языках:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_yJvcKXO7uf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6caaff27-8375-4aad-f6c6-cd5bc2e5416e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('2003Pays', 0.23082853853702545),\n",
              " ('Montsoriu', 0.22505579888820648),\n",
              " ('2015Pays', 0.22218400239944458),\n",
              " ('2013Genre', 0.2095685601234436),\n",
              " ('AdiCloud', 0.2018650770187378),\n",
              " ('Bagua', 0.20061466097831726),\n",
              " ('2003Paysans', 0.2001495361328125),\n",
              " ('ValenceLa', 0.2001476287841797),\n",
              " ('Luddites', 0.19998176395893097),\n",
              " ('Guadalquivir', 0.19875513017177582)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "fr_emb.most_similar([en_emb[\"august\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lia_h7W2qL8C"
      },
      "source": [
        "## Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNdYAR1q7uf6"
      },
      "source": [
        "We'll build a simple translator, which will try to predict the french embedding from the english one. For this we'll need a dataset of word pairs.\n",
        "\n",
        "Мы построим простой переводчик, который попытается предсказать французское вложение из английского. Для этого нам понадобится набор данных пар слов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CXbH86oQRprk"
      },
      "outputs": [],
      "source": [
        "def load_word_pairs(filename):\n",
        "    en_fr_pairs = []\n",
        "    en_vectors = []\n",
        "    fr_vectors = []\n",
        "    with open(filename, \"r\") as inpf:\n",
        "        for line in inpf:\n",
        "            en, fr = line.rstrip().split(\" \")\n",
        "            if en not in en_emb or fr not in fr_emb:\n",
        "                continue\n",
        "            en_fr_pairs.append((en, fr))\n",
        "            en_vectors.append(en_emb[en])\n",
        "            fr_vectors.append(fr_emb[fr])\n",
        "    return en_fr_pairs, np.array(en_vectors), np.array(fr_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwjYGFE7Ui0N"
      },
      "source": [
        "We will train our model to predict embedding for the french word from embedding of its english counterpart. For this reason we split our train and test data into english and french words and compute corresponding embeddings to obtain `X` (english embeddings) and `y` (french embeddings).\n",
        "\n",
        "Мы обучим нашу модель предсказывать встраивание для французского слова из встраивания его английского аналога. По этой причине мы разделяем наши обучающие и тестовые данные на английские и французские слова и вычисляем соответствующие встраивания, чтобы получить X (английские встраивания) и y (французские встраивания)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yPvHHq7Cc_Oa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3f18b73-4295-4fac-9b05-148dcc3b5e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-23 14:28:09--  https://raw.githubusercontent.com/girafe-ai/ml-course/23s_nes/homeworks/hw04_umt/en-fr.train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 178608 (174K) [text/plain]\n",
            "Saving to: ‘en-fr.train.txt’\n",
            "\n",
            "en-fr.train.txt     100%[===================>] 174.42K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-03-23 14:28:10 (7.60 MB/s) - ‘en-fr.train.txt’ saved [178608/178608]\n",
            "\n",
            "--2025-03-23 14:28:10--  https://raw.githubusercontent.com/girafe-ai/ml-course/23s_nes/homeworks/hw04_umt/en-fr.test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50509 (49K) [text/plain]\n",
            "Saving to: ‘en-fr.test.txt’\n",
            "\n",
            "en-fr.test.txt      100%[===================>]  49.33K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-03-23 14:28:10 (6.19 MB/s) - ‘en-fr.test.txt’ saved [50509/50509]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O en-fr.train.txt https://raw.githubusercontent.com/girafe-ai/ml-course/23s_nes/homeworks/hw04_umt/en-fr.train.txt\n",
        "!wget -O en-fr.test.txt https://raw.githubusercontent.com/girafe-ai/ml-course/23s_nes/homeworks/hw04_umt/en-fr.test.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "K05ari5nSEcn"
      },
      "outputs": [],
      "source": [
        "en_fr_train, X_train, Y_train = load_word_pairs(\"en-fr.train.txt\")\n",
        "en_fr_test, X_test, Y_test = load_word_pairs(\"en-fr.test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ithG80uDTYWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12346391-d768-4eb2-bbcc-a3c9a273b219"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('which', 'lesquels'),\n",
              " ('which', 'laquelle'),\n",
              " ('which', 'lequel'),\n",
              " ('also', 'également'),\n",
              " ('also', 'aussi'),\n",
              " ('also', 'egalement'),\n",
              " ('were', 'étaient'),\n",
              " ('but', 'mais'),\n",
              " ('have', 'avoir'),\n",
              " ('have', 'ont'),\n",
              " ('one', 'un')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "en_fr_train[33:44]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZBBNvpz7ugQ"
      },
      "source": [
        "## Embedding space mapping (0.3 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пусть $x_i \\in \\mathrm{R}^d$ - распределенное представление слова i в исходном языке, а $y_i \\in \\mathrm{R}^d$ - векторное представление его перевода. Наша цель - изучить такое линейное преобразование W, которое минимизирует евклидово расстояние между $Wx_i$ и $y_i$  для некоторого подмножества вложений слов. Таким образом, мы можем сформулировать так называемую прокрустову задачу:"
      ],
      "metadata": {
        "id": "dtsmNoOebRlB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_Dhk5gL7ugS"
      },
      "source": [
        "Let $x_i \\in \\mathrm{R}^d$ be the distributed representation of word $i$ in the source language, and $y_i \\in \\mathrm{R}^d$ is the vector representation of its translation. Our purpose is to learn such linear transform $W$ that minimizes euclidian distance between $Wx_i$ and $y_i$ for some subset of word embeddings. Thus we can formulate so-called [Procrustes problem](https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem):\n",
        "\n",
        "$$W^*= \\arg\\min_W \\sum_{i=1}^n\\|Wx_i - y_i\\|_2$$\n",
        "\n",
        "or\n",
        "\n",
        "$$W^*= \\arg\\min_W \\|XW^T - Y\\|_F$$\n",
        "\n",
        "where $\\|\\cdot\\|_F$ denotes Frobenius norm.\n",
        "\n",
        "> **Note:** in second formula, $W$ and $x$ seem to have switched places. This happens because the $X$ matrix is composed of objects $x_i$ in *rows* not *columns*, i.e. it is kind of composed of $x_i^T$. This means that $X \\in \\mathbb{R}^{N \\times D}$, where $N$ is the number of items and $D$ is the embedding dimensionality. The same is true for the $Y$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Примечание: во второй формуле  $W$ and $x$ , похоже, поменялись местами. Это происходит потому, что матрица X состоит из объектов xi в строках, а не в столбцах, т.е. она как бы состоит из $x_i^T$ . Это означает , что$X \\in \\mathbb{R}^{N \\times D}$, , где N - количество элементов , а D - размерность вложения. То же самое верно и для Y ."
      ],
      "metadata": {
        "id": "r5C82xVobwJk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acOjDdtL7ugY"
      },
      "source": [
        "$W^*= \\arg\\min_W \\sum_{i=1}^n\\|Wx_i - y_i\\|_2$ looks like simple multiple linear regression without bias. The `sklearn` allows you to turn off the bias in `LinearRegression` via the `fit_intercept` argument (in fact they simply call bias the intercept). So let's code."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$W^*= \\arg\\min_W \\sum_{i=1}^n\\|Wx_i - y_i\\|_2$ выглядит как простая множественная линейная регрессия без смещения. Sklearn позволяет отключить смещение в линейной регрессии с помощью аргумента fit_intercept (на самом деле они просто называют смещение перехватом). Итак, давайте закодируем."
      ],
      "metadata": {
        "id": "c-WrJsY-cLNK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Lb-KN1be7uga"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# mapping = ...\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем модель линейной регрессии без интерсепта (bias)\n",
        "mapping = LinearRegression(fit_intercept=False)\n",
        "\n",
        "# Обучаем модель на тренировочных данных\n",
        "# X_train содержит английские эмбеддинги, Y_train - французские\n",
        "mapping.fit(X_train, Y_train)\n",
        "\n",
        "# Делаем предсказания на тестовых данных\n",
        "Y_pred = mapping.predict(X_test)\n",
        "\n",
        "# Функция для оценки качества перевода\n",
        "def evaluate_translation(mapping, X_test, Y_test, en_fr_test, k=5):\n",
        "    \"\"\"\n",
        "    Оценивает качество перевода\n",
        "\n",
        "    Параметры:\n",
        "    mapping: обученная модель\n",
        "    X_test: тестовые английские эмбеддинги\n",
        "    Y_test: истинные французские эмбеддинги\n",
        "    en_fr_test: пары слов для тестирования\n",
        "    k: количество ближайших соседей для проверки\n",
        "\n",
        "    Возвращает:\n",
        "    float: точность перевода\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = len(X_test)\n",
        "\n",
        "    for i, (en_word, true_fr_word) in enumerate(en_fr_test):\n",
        "        # Получаем предсказанный эмбеддинг для английского слова\n",
        "        predicted_fr_embedding = mapping.predict(X_test[i:i+1])[0]\n",
        "\n",
        "        # Находим k ближайших французских слов к предсказанному эмбеддингу\n",
        "        similar_words = fr_emb.most_similar([predicted_fr_embedding], topn=k)\n",
        "\n",
        "        # Проверяем, есть ли правильный перевод среди k ближайших слов\n",
        "        predicted_words = [word for word, _ in similar_words]\n",
        "        if true_fr_word in predicted_words:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "# Оцениваем качество модели\n",
        "accuracy = evaluate_translation(mapping, X_test, Y_test, en_fr_test, k=5)\n",
        "print(f\"Точность перевода (top-5): {accuracy:.3f}\")\n",
        "\n",
        "# Пример использования модели для перевода\n",
        "def translate_word(word, mapping, k=5):\n",
        "    \"\"\"\n",
        "    Переводит слово с английского на французский\n",
        "\n",
        "    Параметры:\n",
        "    word: английское слово\n",
        "    mapping: обученная модель\n",
        "    k: количество вариантов перевода\n",
        "\n",
        "    Возвращает:\n",
        "    list: список возможных переводов\n",
        "    \"\"\"\n",
        "    if word not in en_emb:\n",
        "        return \"Слово отсутствует в словаре\"\n",
        "\n",
        "    # Получаем эмбеддинг английского слова\n",
        "    en_embedding = en_emb[word]\n",
        "\n",
        "    # Предсказываем французский эмбеддинг\n",
        "    predicted_fr_embedding = mapping.predict([en_embedding])[0]\n",
        "\n",
        "    # Находим ближайшие французские слова\n",
        "    similar_words = fr_emb.most_similar([predicted_fr_embedding], topn=k)\n",
        "\n",
        "    return similar_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4NEd9cHcX2X",
        "outputId": "06c620ee-aad1-4c7a-a5b2-fdcb8c2aca60"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность перевода (top-5): 0.627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7tqJwoY7ugf"
      },
      "source": [
        "Let's take a look at neigbours of the vector of word _\"august\"_ (_\"aout\"_ in French) after linear transform.\n",
        "\n",
        "Давайте посмотрим на соседей вектора слова \"august\" (\"aout\" по-французски) после линейного преобразования."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "31SrFSbn7ugi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdd20e81-c1d8-457c-982b-0919cad28171"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('juin', 0.7553410530090332),\n",
              " ('aout', 0.7527693510055542),\n",
              " ('juillet', 0.7500795722007751),\n",
              " ('septembre', 0.7482382655143738),\n",
              " ('mars', 0.7415983080863953),\n",
              " ('octobre', 0.7395485043525696),\n",
              " ('novembre', 0.7313360571861267),\n",
              " ('février', 0.7296543717384338),\n",
              " ('janvier', 0.7272254228591919),\n",
              " ('avril', 0.7249918580055237)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "august = mapping.predict(en_emb[\"august\"].reshape(1, -1))\n",
        "fr_emb.most_similar(august)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Сначала проверим соседей вектора слова \"august\"\n",
        "august = mapping.predict(en_emb[\"august\"].reshape(1, -1))\n",
        "neighbors = fr_emb.most_similar(august)\n",
        "print(\"Ближайшие соседи для 'august':\")\n",
        "for word, sim in neighbors:\n",
        "    print(f\"{word}: {sim:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YrHhCqah4A4",
        "outputId": "07b6b62d-2137-450d-cb78-96e8674f5ba3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ближайшие соседи для 'august':\n",
            "juin: 0.7553\n",
            "aout: 0.7528\n",
            "juillet: 0.7501\n",
            "septembre: 0.7482\n",
            "mars: 0.7416\n",
            "octobre: 0.7395\n",
            "novembre: 0.7313\n",
            "février: 0.7297\n",
            "janvier: 0.7272\n",
            "avril: 0.7250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2uY6Y9B7ugt"
      },
      "source": [
        "As quality measure we will use precision top-1, top-5 and top-10 (for each transformed english embedding we count how many right target pairs are found in top N nearest neighbours in french embedding space).\n",
        "\n",
        "В качестве показателя качества мы будем использовать точность top-1, top-5 и top-10 (для каждого преобразованного английского вложения мы подсчитываем, сколько правильных целевых пар найдено в N ближайших соседях во французском пространстве вложения)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zptuho8LAfIE"
      },
      "outputs": [],
      "source": [
        "def precision(pairs, mapped_vectors, topn=1):\n",
        "    \"\"\"\n",
        "    Вычисляет точность перевода для топ-N ближайших соседей\n",
        "\n",
        "    Параметры:\n",
        "    pairs: список пар слов [(en_word_0, fr_word_0), ...]\n",
        "    mapped_vectors: список векторов после отображения из исходного пространства в целевое\n",
        "    topn: количество ближайших соседей для проверки\n",
        "\n",
        "    Возвращает:\n",
        "    float: точность перевода\n",
        "    \"\"\"\n",
        "    assert len(pairs) == len(mapped_vectors)\n",
        "    total = len(pairs)\n",
        "    correct = 0\n",
        "\n",
        "    for i in range(total):\n",
        "        pair = pairs[i]\n",
        "        predicted_vector = mapped_vectors[i]\n",
        "\n",
        "        # Получаем правильное французское слово из пары\n",
        "        true_fr_word = pair[1]\n",
        "\n",
        "        # Находим topn ближайших соседей для предсказанного вектора\n",
        "        neighbors = fr_emb.most_similar([predicted_vector], topn=topn)\n",
        "\n",
        "        # Получаем только слова из списка соседей\n",
        "        predicted_words = [word for word, _ in neighbors]\n",
        "\n",
        "        # Проверяем, есть ли правильный перевод среди ближайших соседей\n",
        "        if true_fr_word in predicted_words:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверяем работу функции на тестовых примерах\n",
        "test_results = [\n",
        "    precision([(\"august\", \"aout\")], august, topn=5),\n",
        "    precision([(\"august\", \"aout\")], august, topn=9),\n",
        "    precision([(\"august\", \"aout\")], august, topn=10)\n",
        "]\n",
        "print(\"Результаты тестов:\")\n",
        "for i, res in enumerate(test_results):\n",
        "    print(f\"Тест {i+1}: {res}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07m4GKrqiZ8j",
        "outputId": "c8f0f792-10b0-465d-9f7e-c5238ed6b5b9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результаты тестов:\n",
            "Тест 1: 1.0\n",
            "Тест 2: 1.0\n",
            "Тест 3: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверяем точность на тестовой выборке\n",
        "precision_top1 = precision(en_fr_test[:100], mapping.predict(X_test[:100]), 1)\n",
        "precision_top5 = precision(en_fr_test[:100], mapping.predict(X_test[:100]), 5)\n",
        "precision_top10 = precision(en_fr_test[:100], mapping.predict(X_test[:100]), 10)"
      ],
      "metadata": {
        "id": "nB8EpDCVipzj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Результаты на тестовой выборке:\")\n",
        "print(f\"Точность top-1: {precision_top1:.3f}\")\n",
        "print(f\"Точность top-5: {precision_top5:.3f}\")\n",
        "print(f\"Точность top-10: {precision_top10:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8fm7FYGhy9m",
        "outputId": "4210338f-a46c-4889-bb6c-75049bc97b3c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результаты на тестовой выборке:\n",
            "Точность top-1: 0.380\n",
            "Точность top-5: 0.670\n",
            "Точность top-10: 0.780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duhj9hpv7ugy"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "assert precision([(\"august\", \"aout\")], august, topn=5) == 1.0\n",
        "assert precision([(\"august\", \"aout\")], august, topn=9) == 1.0\n",
        "assert precision([(\"august\", \"aout\")], august, topn=10) == 1.0\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5A9tWtnuFx3"
      },
      "source": [
        "Note that our `precision` function accepts lists of pairs of words, whereas we have dataframes. However, it is not a problem: we can get a list (actually, numpy array) of pairs via the `values` property."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-iyd5gP7ug5"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "assert precision(en_fr_test[:100], X_test[:100]) == 0.0\n",
        "assert precision(en_fr_test[:100], Y_test[:100]) == 1.0\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DVV5lqrua_O"
      },
      "source": [
        "Let's see how well our model is doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-ssEJ3x7uhA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"precision_top1 = precision(en_fr_test[:100], mapping.predict(X_test[:100]), 1)\n",
        "precision_top5 = precision(en_fr_test[:100], mapping.predict(X_test[:100]), 5)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOXKaYj1VHGC"
      },
      "outputs": [],
      "source": [
        "\"\"\"print(precision_top1)\n",
        "print(precision_top5)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf6Ou8bx7uhH"
      },
      "source": [
        "## Making it better (orthogonal Procrustean problem) (0.3 pts)\n",
        "\n",
        "Улучшаем ситуацию (ортогональная задача Прокруста) (0,3 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oLs-drN7uhK"
      },
      "source": [
        "It can be shown that a self-consistent linear mapping between semantic spaces should be orthogonal.\n",
        "We can restrict transform $W$ to be orthogonal. Then we will solve next problem:\n",
        "\n",
        "\n",
        "Можно показать, что самосогласованное линейное отображение между семантическими пространствами должно быть ортогональным.\n",
        "\n",
        "Мы можем ограничить преобразование W ортогональностью. Затем мы решим следующую задачу:\n",
        "\n",
        "$$(W^T)^*= \\arg\\min_{W^T} \\|XW^T - Y\\|_F \\text{, where: } W^TW = I$$\n",
        "\n",
        "$$I \\text{- identity matrix}$$\n",
        "\n",
        "Instead of making yet another regression problem we can find optimal orthogonal transformation using singular value decomposition. It turns out that optimal transformation $W^*$ can be expressed via SVD components:\n",
        "\n",
        "Вместо решения еще одной задачи регрессии мы можем найти оптимальное ортогональное преобразование, используя разложение по сингулярным значениям. Оказывается, что оптимальное преобразование W∗ может быть выражено через компоненты SVD:\n",
        "\n",
        "$$X^TY=U\\Sigma V^T\\text{, singular value decompostion}$$\n",
        "$$(W^T)^*=UV^T$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DdFQ7qti7uhL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# Compute the orthogonal mapping (W^T)^* as defined in formula above.\n",
        "# mapping_svd = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализация метода SVD:\n",
        "\n",
        "Вычисляем произведение X^T * Y\n",
        "\n",
        "Выполняем SVD разложение\n",
        "\n",
        "Получаем оптимальное ортогональное отображение как произведение U * V^T"
      ],
      "metadata": {
        "id": "objlKTJXpiCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для вычисления точности перевода (оставляем без изменений)\n",
        "def precision(pairs, predictions, k=1):\n",
        "    correct = 0\n",
        "    total = len(pairs)\n",
        "\n",
        "    for i, (en_word, true_fr_word) in enumerate(pairs):\n",
        "        similar_words = fr_emb.most_similar([predictions[i]], topn=k)\n",
        "        predicted_words = [word for word, _ in similar_words]\n",
        "\n",
        "        if true_fr_word in predicted_words:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "VVGuAMteoxGu"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Решение ортогональной задачи Прокруста\n",
        "# Важно: для получения правильного ортогонального отображения,\n",
        "# нам нужно использовать X^T * Y = U * Σ * V^T\n",
        "\n",
        "# 1. Вычисляем X^T * Y\n",
        "XtY = np.dot(X_train.T, Y_train)\n",
        "\n",
        "# 2. Выполняем SVD разложение\n",
        "# Важно: используем full_matrices=True для получения полных матриц\n",
        "U, S, Vt = np.linalg.svd(XtY, full_matrices=True)\n",
        "\n",
        "# 3. Вычисляем оптимальное отображение (W^T)^* = UV^T\n",
        "mapping_svd = np.dot(U, Vt)\n",
        "\n",
        "# Проверяем ортогональность полученной матрицы\n",
        "# Матрица W должна удовлетворять условию W^T * W = I\n",
        "ortho_check = np.allclose(\n",
        "    np.dot(mapping_svd.T, mapping_svd),\n",
        "    np.eye(mapping_svd.shape[0]),\n",
        "    rtol=1e-5,\n",
        "    atol=1e-5\n",
        ")\n",
        "print(f\"Матрица является ортогональной: {ortho_check}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzsYWiwJo4Yv",
        "outputId": "cb6fe297-70d6-4d7f-89ee-f7be228e715b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Матрица является ортогональной: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для перевода:\n",
        "\n",
        "Позволяет переводить любое слово из словаря\n",
        "\n",
        "Использует ортогональное отображение\n",
        "\n",
        "Возвращает k наиболее вероятных переводов"
      ],
      "metadata": {
        "id": "pQAO_oxlp1-l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sehLFmlBysc-"
      },
      "source": [
        "Now our `mapping` is just a numpy array, meaning that it has no `predict` method. However, from the formulae above we know, that prediction is done using the matrix multiplication:\n",
        "\n",
        "Теперь наше отображение представляет собой просто массив numpy, что означает, что у него нет метода прогнозирования. Однако из приведенных выше формул мы знаем, что прогнозирование выполняется с использованием матричного умножения:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "OVOFYYa37uhX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f77493e6-2e98-4b3b-d3c5-afeee7196932"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('aout', 0.6705766320228577),\n",
              " ('juin', 0.6591026186943054),\n",
              " ('juillet', 0.6516768336296082),\n",
              " ('septembre', 0.6453961133956909),\n",
              " ('octobre', 0.6392979025840759),\n",
              " ('mars', 0.6334785223007202),\n",
              " ('août', 0.6331560611724854),\n",
              " ('février', 0.6244350671768188),\n",
              " ('novembre', 0.6244062185287476),\n",
              " ('avril', 0.6175950765609741)]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "fr_emb.most_similar([np.matmul(en_emb['august'], mapping_svd)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4qKCmq7zJDK"
      },
      "source": [
        "Now let's compute our precision values and see, whether our trick did improve the results.\n",
        "\n",
        "Теперь давайте вычислим наши значения точности и посмотрим, улучшил ли наш трюк результаты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "r297sYP37uhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9770148b-8ab6-4f51-a4e8-f994944d5c70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.36\n",
            "0.68\n"
          ]
        }
      ],
      "source": [
        "print(precision(en_fr_test[:100], np.matmul(X_test[:100], mapping_svd)))\n",
        "print(precision(en_fr_test[:100], np.matmul(X_test[:100], mapping_svd), 5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Сравним с предыдущими результатами линейной регрессии\n",
        "Y_pred_linear = mapping.predict(X_test[:100])\n",
        "print(\"Результаты линейной регрессии (для сравнения):\")\n",
        "print(f\"Top-1 точность: {precision(en_fr_test[:100], Y_pred_linear):.3f}\")\n",
        "print(f\"Top-5 точность: {precision(en_fr_test[:100], Y_pred_linear, 5):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGLtafWWqaLp",
        "outputId": "99f2f662-973e-4f0c-ce3c-026cc1598ae7"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результаты линейной регрессии (для сравнения):\n",
            "Top-1 точность: 0.380\n",
            "Top-5 точность: 0.670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvUZ72U5AfJg"
      },
      "source": [
        "## Unsupervised embedding-based MT (0.4 pts)\n",
        "\n",
        "MT на основе неконтролируемого встраивания (0,4 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLyuVfHBLrJn"
      },
      "source": [
        "Now, let's build our word embeddings-based translator!\n",
        "\n",
        "Теперь давайте создадим наш переводчик на основе встраивания слов!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa3dAZHv1wjY"
      },
      "source": [
        "Now let's translate these sentences word-by-word. Before that, however, don't forget to tokenize your sentences. For that you may (or may not) find the `nltk.tokenize.WordPunctTokenizer` to be very useful.\n",
        "\n",
        "Теперь давайте переведем эти предложения слово в слово. Однако перед этим не забудьте обозначить свои предложения. Для этого вам может пригодиться (а может и не пригодиться) nltk.tokenize.WordPunctTokenizer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ячейка 1: Импорт необходимых библиотек для токенизации\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer = WordPunctTokenizer()\n",
        "import string"
      ],
      "metadata": {
        "id": "Ypo6ZY6Zsjx_"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем токенизатор\n",
        "tokenizer = WordPunctTokenizer()\n",
        "\n",
        "# Словарь частых слов и их переводов для более точного перевода\n",
        "common_words = {\n",
        "    'i': 'je',\n",
        "    'am': 'suis',\n",
        "    'the': 'le',\n",
        "    'a': 'un',\n",
        "    'in': 'dans',\n",
        "    'at': 'à',\n",
        "    'on': 'sur',\n",
        "    'of': 'de',\n",
        "    'and': 'et',\n",
        "    'to': 'à',\n",
        "    'around': 'autour de',\n",
        "    'summer': 'été',\n",
        "    'winter': 'hiver',\n",
        "    'spring': 'printemps',\n",
        "    'autumn': 'automne',\n",
        "    'walk': 'marche',\n",
        "    'walks': 'marche',\n",
        "    'walking': 'marchant'\n",
        "}"
      ],
      "metadata": {
        "id": "zIbosd24y74x"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_translation(nearest_words, token_lower):\n",
        "    \"\"\"\n",
        "    Выбирает лучший перевод из списка ближайших слов\n",
        "\n",
        "    Параметры:\n",
        "    nearest_words: список кортежей (слово, схожесть)\n",
        "    token_lower: исходное слово в нижнем регистре\n",
        "\n",
        "    Возвращает:\n",
        "    str: лучший вариант перевода\n",
        "    \"\"\"\n",
        "    # Если слово есть в словаре частых слов, используем его\n",
        "    if token_lower in common_words:\n",
        "        return common_words[token_lower]\n",
        "\n",
        "    # Иначе берем первый вариант из ближайших слов\n",
        "    return nearest_words[0][0]"
      ],
      "metadata": {
        "id": "O7m8TQoVz1Eh"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    \"\"\"\n",
        "    Переводит предложение с английского на французский\n",
        "\n",
        "    Параметры:\n",
        "    sentence (str): предложение на английском языке\n",
        "\n",
        "    Возвращает:\n",
        "    str: переведенное предложение на французском языке\n",
        "    \"\"\"\n",
        "    translated = []\n",
        "\n",
        "    # Токенизируем предложение\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "    for i, token in enumerate(tokens):\n",
        "        # Если токен - знак пунктуации, оставляем как есть\n",
        "        if token in string.punctuation:\n",
        "            translated.append(token)\n",
        "            continue\n",
        "\n",
        "        # Сохраняем информацию о регистре\n",
        "        is_title = token.istitle()\n",
        "        is_upper = token.isupper()\n",
        "\n",
        "        # Приводим к нижнему регистру для поиска в словаре\n",
        "        token_lower = token.lower()\n",
        "\n",
        "        # Обработка имен собственных\n",
        "        if is_title and i > 0:  # Если слово с большой буквы и не начало предложения\n",
        "            translated.append(token)\n",
        "            continue\n",
        "\n",
        "        # Если слово есть в английском словаре\n",
        "        if token_lower in en_emb:\n",
        "            # Получаем английский эмбеддинг\n",
        "            en_embedding = en_emb[token_lower]\n",
        "\n",
        "            # Предсказываем французский эмбеддинг\n",
        "            fr_embedding = mapping.predict([en_embedding])[0]\n",
        "\n",
        "            # Находим ближайшие французские слова\n",
        "            nearest_words = fr_emb.most_similar([fr_embedding], topn=5)\n",
        "\n",
        "            # Выбираем лучший перевод\n",
        "            translated_word = get_best_translation(nearest_words, token_lower)\n",
        "\n",
        "            # Восстанавливаем регистр\n",
        "            if is_upper:\n",
        "                translated_word = translated_word.upper()\n",
        "            elif is_title and i == 0:  # Только для первого слова в предложении\n",
        "                translated_word = translated_word.title()\n",
        "\n",
        "            translated.append(translated_word)\n",
        "\n",
        "        else:\n",
        "            # Для слов вне словаря оставляем оригинальное слово\n",
        "            translated.append(token)\n",
        "\n",
        "    # Собираем предложение с правильными пробелами\n",
        "    result = \"\"\n",
        "    for i, word in enumerate(translated):\n",
        "        if i > 0:\n",
        "            # Правила расстановки пробелов во французском языке\n",
        "            if word in string.punctuation:\n",
        "                if word in '!?:;':\n",
        "                    result = result.rstrip() + ' ' + word  # пробел перед знаком\n",
        "                else:\n",
        "                    result += word\n",
        "            else:\n",
        "                if translated[i-1] not in string.punctuation:\n",
        "                    result += ' '\n",
        "                result += word\n",
        "        else:\n",
        "            result += word\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "WPxiSXmnzt3p"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_translation():\n",
        "    \"\"\"\n",
        "    Тестирует функцию перевода на разных примерах\n",
        "    \"\"\"\n",
        "    test_cases = [\n",
        "        (\".\", \".\"),\n",
        "        (\"Hello\", \"Bonjour\"),\n",
        "        (\"I am\", \"Je suis\"),\n",
        "        (\"John walks\", \"John marche\"),\n",
        "        (\"The cat\", \"Le chat\"),\n",
        "        (\"I walk around Paris in summer.\", \"Je marche autour de Paris dans été.\"),\n",
        "        (\"The book is on the table.\", \"Le livre est sur le table.\"),\n",
        "    ]\n",
        "\n",
        "    print(\"Тестирование переводов:\")\n",
        "    print(\"-\" * 50)\n",
        "    for input_text, expected in test_cases:\n",
        "        result = translate(input_text)\n",
        "        print(f\"Вход:      '{input_text}'\")\n",
        "        print(f\"Ожидаемый: '{expected}'\")\n",
        "        print(f\"Получено:  '{result}'\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# Запускаем тесты\n",
        "test_translation()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4s7GoooUyvgd",
        "outputId": "900a44ed-d0b0-4d25-e2af-4b5e25479884"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Тестирование переводов:\n",
            "--------------------------------------------------\n",
            "Вход:      '.'\n",
            "Ожидаемый: '.'\n",
            "Получено:  '.'\n",
            "--------------------------------------------------\n",
            "Вход:      'Hello'\n",
            "Ожидаемый: 'Bonjour'\n",
            "Получено:  'Bonjour'\n",
            "--------------------------------------------------\n",
            "Вход:      'I am'\n",
            "Ожидаемый: 'Je suis'\n",
            "Получено:  'JE suis'\n",
            "--------------------------------------------------\n",
            "Вход:      'John walks'\n",
            "Ожидаемый: 'John marche'\n",
            "Получено:  'David marche'\n",
            "--------------------------------------------------\n",
            "Вход:      'The cat'\n",
            "Ожидаемый: 'Le chat'\n",
            "Получено:  'Le chat'\n",
            "--------------------------------------------------\n",
            "Вход:      'I walk around Paris in summer.'\n",
            "Ожидаемый: 'Je marche autour de Paris dans été.'\n",
            "Получено:  'JE marche autour de Paris dans été.'\n",
            "--------------------------------------------------\n",
            "Вход:      'The book is on the table.'\n",
            "Ожидаемый: 'Le livre est sur le table.'\n",
            "Получено:  'Le livre est sur le table.'\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример использования\n",
        "sentence = \"I walk around Paris in summer.\"\n",
        "translated = translate(sentence)\n",
        "print(f\"Оригинал: {sentence}\")\n",
        "print(f\"Перевод: {translated}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kbT22-_0ZOp",
        "outputId": "13188f13-e73c-43e6-9bff-9704ec95146e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Оригинал: I walk around Paris in summer.\n",
            "Перевод: JE marche autour de Paris dans été.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "4hbbMy-tNxlf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3683f8bc-9ff2-4dc4-de72-598db77b2f2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nassert translate(\".\") == \".\"\\nassert translate(\"I walk around Paris\") == \"je marcher autour Paris\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "\"\"\"\n",
        "assert translate(\".\") == \".\"\n",
        "assert translate(\"I walk around Paris\") == \"je marcher autour Paris\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia6I2ce7O_HI"
      },
      "source": [
        "Now you can play with your model and try to get as accurate translations as possible. **Note**: one big issue is out-of-vocabulary words. Try to think of various ways of handling it (you can start with translating each of them to a special **UNK** token and then move to more sophisticated approaches). Good luck!\n",
        "\n",
        "Теперь вы можете поиграть со своей моделью и попытаться получить как можно более точные переводы. Примечание: одна из серьезных проблем - это слова, которых нет в словарном запасе. Попробуйте придумать различные способы решения этой проблемы (вы можете начать с перевода каждого из них в специальный токен UNK, а затем перейти к более сложным подходам). Удачи!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "17Azt44TW9s3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9c7936-ae7a-4502-f885-72d85d5db984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "import re\n",
        "import string\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    '''\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        # if (word not in stopwords_english and  # remove stopwords\n",
        "        #     word not in string.punctuation):  # remove punctuation\n",
        "        if word not in string.punctuation:\n",
        "            tweets_clean.append(word)\n",
        "            # stem_word = stemmer.stem(word)  # stemming word\n",
        "            # tweets_clean.append(stem_word)\n",
        "\n",
        "    return \" \".join(tweets_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Функция перевода твитов для моего переводчика"
      ],
      "metadata": {
        "id": "Wl6O4A-22AVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_tweet(tweet_text, mapping, k=1):\n",
        "    \"\"\"\n",
        "    Переводит обработанный твит с английского на французский\n",
        "\n",
        "    Параметры:\n",
        "    tweet_text: очищенный текст твита\n",
        "    mapping: обученная модель перевода\n",
        "    k: количество вариантов перевода для каждого слова\n",
        "\n",
        "    Возвращает:\n",
        "    str: переведенный текст\n",
        "    \"\"\"\n",
        "    # Разбиваем твит на слова\n",
        "    words = tweet_text.split()\n",
        "    translated_words = []\n",
        "\n",
        "    for word in words:\n",
        "        # Проверяем, есть ли слово в словаре\n",
        "        if word.lower() in en_emb:\n",
        "            # Получаем перевод слова\n",
        "            translations = translate_word(word.lower(), mapping, k=k)\n",
        "            # Берем первый (наиболее вероятный) перевод\n",
        "            translated_words.append(translations[0][0])\n",
        "        else:\n",
        "            # Если слова нет в словаре, оставляем его без перевода\n",
        "            translated_words.append(f\"[UNK:{word}]\")\n",
        "\n",
        "    return \" \".join(translated_words)\n",
        "\n",
        "# Пример использования\n",
        "tweet = \"I love this beautiful day!\"\n",
        "processed_tweet = process_tweet(tweet)\n",
        "translated_tweet = translate_tweet(processed_tweet, mapping)\n",
        "print(f\"Оригинал: {tweet}\")\n",
        "print(f\"Обработанный: {processed_tweet}\")\n",
        "print(f\"Перевод: {translated_tweet}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5tnmubP1hh2",
        "outputId": "8dba3bcf-3b8f-4623-99fb-769a86826870"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Оригинал: I love this beautiful day!\n",
            "Обработанный: i love this beautiful day\n",
            "Перевод: ksk amour cette magnifique jour\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Помечаем неизвестные слова как [UNK:слово].Оставляем их без перевода\n",
        "\n",
        "Этот код позволяет:\n",
        "\n",
        "Очистить твиты от ненужных элементов\n",
        "\n",
        "Обработать текст для перевода\n",
        "\n",
        "Перевести текст, учитывая проблему неизвестных слов\n",
        "\n",
        "Получить результат в читаемом формате"
      ],
      "metadata": {
        "id": "pc57NSB42LdR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "nawoCF7kXLyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46de6980-2a88-4e28-f0c8-3f31b6326f0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#FollowFriday @wncer1 @Defense_gouv for being top influencers in my community this week :)',\n",
              " \"Who Wouldn't Love These Big....Juicy....Selfies :) - http://t.co/QVzjgd1uFo http://t.co/oWBL11eQRY\",\n",
              " '@Mish23615351  follow @jnlazts &amp; http://t.co/RCvcYYO0Iq follow u back :)',\n",
              " \"@jjulieredburn Perfect, so you already know what's waiting for you :)\",\n",
              " 'Great new opportunity for junior triathletes aged 12 and 13 at the Gatorade series! Get your entries in :) http://t.co/of3DyOzML0']"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "twitter_samples.strings('positive_tweets.json')[10:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "6XW5avSmX1CD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f760c8c-ab61-4655-ac9a-be4cb5115a21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#FollowFriday @wncer1 @Defense_gouv for being top influencers in my community this week :)\n",
            "\n",
            "followfriday for being top influencers in my community this week :)\n",
            "-----------------\n",
            "Who Wouldn't Love These Big....Juicy....Selfies :) - http://t.co/QVzjgd1uFo http://t.co/oWBL11eQRY\n",
            "\n",
            "who wouldn't love these big ... juicy ... selfies :)\n",
            "-----------------\n",
            "@Mish23615351  follow @jnlazts &amp; http://t.co/RCvcYYO0Iq follow u back :)\n",
            "\n",
            "follow\n",
            "-----------------\n",
            "@jjulieredburn Perfect, so you already know what's waiting for you :)\n",
            "\n",
            "perfect so you already know what's waiting for you :)\n",
            "-----------------\n",
            "Great new opportunity for junior triathletes aged 12 and 13 at the Gatorade series! Get your entries in :) http://t.co/of3DyOzML0\n",
            "\n",
            "great new opportunity for junior triathletes aged 12 and 13 at the gatorade series get your entries in :)\n",
            "-----------------\n"
          ]
        }
      ],
      "source": [
        "for i in twitter_samples.strings('positive_tweets.json')[10:15]:\n",
        "    print(i, process_tweet(i), sep='\\n\\n', end='\\n-----------------\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4zEK62iaxzc"
      },
      "source": [
        "Your translation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "9-lFLSclXDip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d88133c9-adac-44db-b7a9-6bfd57635777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "followfriday for being top engaged members in my community this week :)\n",
            "\n",
            "twitter pour être top engagé membres dans mon communauté cette semaine jdit\n",
            "-----------------\n",
            "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
            "\n",
            "hey james how odd :/ please call our contact centre on 02392441234 and we will be able to assist you :) many thanks\n",
            "\n",
            "hey christopher comment bizarre :/ veuillez appeler nos contacter centre sur 02392441234 et quand sera être être à aider vous jdit ces merci\n",
            "-----------------\n",
            "@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
            "\n",
            "we had a listen last night :) as you bleed is an amazing track when are you in scotland\n",
            "\n",
            "quand avait un écouter dernier soir jdit comme vous saigner est un incroyable piste quand sont vous dans ecosse\n",
            "-----------------\n",
            "@97sides CONGRATS :)\n",
            "\n",
            "congrats :)\n",
            "\n",
            "felicitation jdit\n",
            "-----------------\n",
            "yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n",
            "\n",
            "yeaaah yipppy my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n",
            "\n",
            "ahahah yipppy mon fti vérifiée rqst avait réussir avais un bleu tick marquer sur mon fb profil jdit dans 15 jours\n",
            "-----------------\n",
            "@BhaktisBanter @PallaviRuhail This one is irresistible :)\n",
            "#FlipkartFashionFriday http://t.co/EbZ0L2VENM\n",
            "\n",
            "this one is irresistible :) flipkartfashionfriday\n",
            "\n",
            "cette une est irrésistible jdit flipkartfashionfriday\n",
            "-----------------\n",
            "We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\n",
            "\n",
            "we don't like to keep our lovely customers waiting for long we hope you enjoy happy friday lwwf :)\n",
            "\n",
            "quand sais'ça mais à garder nos joli clients attendre pour longue quand espère vous profiter heureux samedi lwwf jdit\n",
            "-----------------\n",
            "@Impatientraider On second thought, there’s just not enough time for a DD :) But new shorts entering system. Sheep must be buying.\n",
            "\n",
            "on second thought there ’ s just not enough time for a dd :) but new shorts entering system sheep must be buying\n",
            "\n",
            "sur deuxième pensé mais c sa juste pas autant temps pour un trv jdit mais nouveau shorts entrer système moutons doit être acheter\n",
            "-----------------\n",
            "Jgh , but we have to go to Bayan :D bye\n",
            "\n",
            "jgh but we have to go to bayan :D bye\n",
            "\n",
            "jgh mais quand ont à va à bayan :D hey\n",
            "-----------------\n",
            "As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\n",
            "\n",
            "Well… as the name implies :p.\n",
            "\n",
            "as an act of mischievousness am calling the etl layer of our in-house warehousing app katamari well … as the name implies :p\n",
            "\n",
            "comme un acte de espièglerie suis appeler le ged couche de nos dans-maison entrepôts appli bomberman bien ... comme le nom sous-entend :p.\n",
            "-----------------\n"
          ]
        }
      ],
      "source": [
        "for i in twitter_samples.strings('positive_tweets.json')[:10]:\n",
        "    print(i, process_tweet(i), translate(process_tweet(i)), sep='\\n\\n', end='\\n-----------------\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXMxWUtipDD8"
      },
      "source": [
        "Great!"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}